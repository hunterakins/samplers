"""
Description:

Date:

Author: Hunter Akins

Institution: Scripps Institution of Oceanography, UC San Diego
"""

import numpy as np
from matplotlib import pyplot as plt
from matplotlib import rc
rc('text', usetex=True)
import matplotlib
matplotlib.rcParams['mathtext.fontset'] = 'stix'
matplotlib.rcParams['font.family'] = 'STIXGeneral'
from samplers.likelihoods import GaussianLikelihood, MultimodalGaussianLikelihood, RosenbrockTwoD
from samplers.rjpt import AdaptiveTransDPTSampler, Chain, ChainParams
from samplers.helpers import *

pics_folder = '/home/hunter/research/samplers/notes/pics/'

def generate_polynomial_samples(tgrid, coeffs):
    """
    Generate samples from a polynomial
    """
    return np.polyval(coeffs, tgrid)

def generate_polynomial_samples_with_noise(tgrid, coeffs, noise_std):
    """
    Generate samples from a polynomial
    """
    return np.polyval(coeffs, tgrid) + np.random.normal(0, noise_std, len(tgrid))

def f_log_prior(x):
    """
    Return log prior probability of x
    Assume regardless it is a zero mean unit variance Gaussian?
    """
    dim = int(x[0])
    xvals = x[1:dim+1]
    log_prior = -dim*np.log(2*np.pi)/2 - np.sum(xvals**2)/2
    return log_prior

def get_log_lh(y, tgrid, noise_std):
    """
    Fix msmts y
    """
    def f_log_lh(x):
        """
        x is polynomial coefficients plus the dimesnion
        """
        dim = int(x[0])
        xvals = x[1:dim+1]
        y_pred = np.polyval(xvals, tgrid)
        log_lh = -dim*np.log(2*np.pi*noise_std**2)/2 - np.sum((y-y_pred)**2)/(2*noise_std**2)
        return log_lh
    return f_log_lh

def f_proposal(prop_cov):
    """
    Generate a random multivariate Gaussian sample according 
    to the proposal covariance prop_cov (zero mean)
    """
    if prop_cov.size > 1:
        val =  np.random.multivariate_normal(np.zeros(prop_cov.shape[0]), prop_cov)
        return val, None
    else:
        val = np.random.randn()*np.sqrt(prop_cov)
        log_prob = -np.log(2*np.pi*prop_cov)/2 - val**2/(2*prop_cov)
        return val, log_prob

def f_log_gprime(x, **kwargs):
    """
    Return the log of the probability that scalar x
    was generated by the proposal distribution
    The proposal is gaussian with unit standard deviation unless specified in kwargs
    """
    if kwargs.get('sigma') is None:
        sigma = 1
    else:
        sigma = kwargs.get('sigma')
    return -np.log(2*np.pi*sigma**2) - (x/sigma)**2/2

def f_prior(dim):
    """
    Generate a random sample from the prior
    """
    cov = np.eye(dim)
    return f_proposal(cov)[0]

def polynomial_regression():
    """
    First generate some data and introduce some noise
    """
    snr_db = 20
    #np.random.seed(1)
    tgrid = np.linspace(-1, 1, 21)
    coeffs = np.array([.9, -.3, +1.1, -1.2]) # polynomial order is 3, dimension is 4
    y = np.polyval(coeffs, tgrid)
    noise_var = (np.var(y)/(10**(snr_db/10)))
    noise_std = np.sqrt(noise_var)
    y_true = np.copy(y)
    y += noise_std*np.random.randn(tgrid.size)
    fig, ax = plt.subplots()
    ax.plot(tgrid, y_true, 'r', label='true model')
    ax.plot(tgrid, y, 'o', label='msmt')
    ax.set_xlabel('Time')
    ax.set_ylabel('y')

    f_log_lh = get_log_lh(y, tgrid, noise_std) # set log lh fun

    plt.figure()
    for i in range(4):
        lh_vals = np.zeros(101)
        x = np.zeros(5)
        delta_vals = np.linspace(-2.0, 2.0, 101)
        for j in range(delta_vals.size):
            delta = delta_vals[j]
            x[1:] = coeffs.copy()
            x[0] = 4
            x[i+1] += delta
            lh = np.exp(f_log_lh(x))
            lh_vals[j] = lh
        plt.plot(delta_vals, lh_vals, label='coeff {}'.format(i+1))
    plt.xlabel('diff. between cand. and true val.')
    plt.ylabel('likelihood')
    plt.grid()
    plt.legend()


    """
    Tune the proposal scale factor sd_arr
    """
    dim_list = [2,3,4,5,6] # dimensions to try...
    move_probs = [[1.0, 0.0],[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.0, 1.0]]
    num_chains = 10
    Tmax = 100
    temp_ladder = np.exp(np.linspace(0, np.log(Tmax), num_chains))
    beta_arr  = 1/temp_ladder # convert to beta
    update_after_burn = True
    eps = 1e-7

    sampler = AdaptiveTransDPTSampler(move_probs, dim_list, beta_arr, f_log_prior, f_log_lh, f_proposal, f_log_gprime)
    prop_covs = sampler.tune_proposal(500, f_prior)
    N_samples = 20000
    nu = N_samples # this means no adaptive update
    N_burn_in = 3000
    swap_interval = 1 # propose chain swaps every step
    sd_arr = 2.4**2 / np.array(dim_list)
    prop_cov_arr = 2*noise_std**2 / beta_arr

    sigma_scale = 0.1
    prop_cov_arr *= sigma_scale

    """ 
    Now run 
    """
    sampler.initialize_chains(N_samples, N_burn_in, nu, f_prior, update_after_burn, swap_interval, prop_covs)
    chain_list = sampler.chain_list

    sampler.sample()
    chain_list = sampler.chain_list
    cold_chain = chain_list[0]
    best_chain_ind = np.argmax(cold_chain.log_probs) + 1
    map_x = cold_chain.samples[:, np.argmax(cold_chain.log_probs)]
    dim = int(map_x[0])
    map_coeff = map_x[1:dim+1]
    print('MAP coeff: {}'.format(map_coeff))
    cold_samples = cold_chain.samples
    print(1/cold_chain.params.beta)
    cold_samples = cold_samples[:, N_burn_in:]
    vals = np.zeros((tgrid.size, cold_samples.shape[1]))
    for i in range(cold_samples.shape[1]):
        dim = int(cold_samples[0, i])
        vals[:, i] = np.polyval(cold_samples[1:dim+1, i], tgrid)
    mean_val = np.mean(vals, axis=1)
    std_val = np.std(vals, axis=1)
    ax.plot(tgrid, np.polyval(map_coeff, tgrid), 'k--', alpha=1, label='map')
    ax.fill_between(tgrid, mean_val-2*std_val, mean_val+2*std_val, alpha=0.2, label='95% credible interval')
    ax.legend()

    sampler.diagnostic_plot()
    plt.show()

def tune_sd():
    """
    Tune the proposal scale factor sd_arr
    Results is that sigma_scale = 0.1 gives good proposal acceptance
    """
    snr_db = 20
    tgrid = np.linspace(-1, 1, 21)
    coeffs = np.array([.1, -.3, +1.1, -1.2]) # polynomial order is 3, dimension is 4
    y = np.polyval(coeffs, tgrid)
    noise_std = np.sqrt(np.var(y)/10**(snr_db/10))
    y_true = np.copy(y)
    y += noise_std*np.random.randn(tgrid.size)

    f_log_lh = get_log_lh(y, tgrid, noise_std) # set log lh fun

    dim_list = [2,3,4,5,6] # dimensions to try...
    move_probs = [[1.0, 0.0],[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.0, 1.0]]
    num_chains = 6
    Tmax = 100
    temp_ladder = np.exp(np.linspace(0, np.log(Tmax), num_chains))
    beta_arr  = 1/temp_ladder # convert to beta
    update_after_burn = False
    nu = 100
    eps = 1e-7

    sampler = AdaptiveTransDPTSampler(move_probs, dim_list, beta_arr, f_log_prior, f_log_lh, f_proposal, f_log_gprime)
    N_samples = 20000
    N_burn_in = 3000
    swap_interval = 1 # propose chain swaps every step
    sd_arr = 2.4**2 / np.array(dim_list)
    prop_cov_arr = 2*noise_std**2 / beta_arr
    sampler.tune_proposal(500, f_prior)

def example_comparison_script():
    """
    Do a transdimensional regression on the polynomial from the other examples 
    I used MH to do
    """
    """
    First generate some data and introduce some noise
    """
    snr_db = 10
    #np.random.seed(1)
    N = 100
    tgrid = np.linspace(-1, 1, N)
    coeffs = np.load('m_true.npy')
    print('true dim', coeffs.size)
    #coeffs = np.array([.9, -.3, +1.1, -1.2]) # polynomial order is 3, dimension is 4
    y = np.polyval(coeffs, tgrid)
    noise_var = (np.var(y)/(10**(snr_db/10)))
    noise_std = np.sqrt(noise_var)
    y_true = np.copy(y)
    y += noise_std*np.random.randn(tgrid.size)
    fig, ax = plt.subplots()
    ax.plot(tgrid, y_true, 'r', label='true model')
    ax.plot(tgrid, y, 'o', label='msmt')
    ax.set_xlabel('Time')
    ax.set_ylabel('y')

    f_log_lh = get_log_lh(y, tgrid, noise_std) # set log lh fun

    plt.figure()
    for i in range(4):
        lh_vals = np.zeros(101)
        x = np.zeros(coeffs.size+1)
        delta_vals = np.linspace(-2.0, 2.0, 101)
        for j in range(delta_vals.size):
            delta = delta_vals[j]
            x[1:] = coeffs.copy()
            x[0] = coeffs.size
            x[i+1] += delta
            lh = np.exp(f_log_lh(x))
            lh_vals[j] = lh
        plt.plot(delta_vals, lh_vals, label='coeff {}'.format(i+1))
    plt.xlabel('diff. between cand. and true val.')
    plt.ylabel('likelihood')
    plt.grid()
    plt.legend()


    """
    Tune the proposal scale factor sd_arr
    """
    dim_list = [2,3,4,5,6] # dimensions to try...
    move_probs = [[1.0, 0.0],[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.0, 1.0]]
    num_chains = 10
    Tmax = 100
    temp_ladder = np.exp(np.linspace(0, np.log(Tmax), num_chains))
    beta_arr  = 1/temp_ladder # convert to beta
    update_after_burn = True
    eps = 1e-7

    sampler = AdaptiveTransDPTSampler(move_probs, dim_list, beta_arr, f_log_prior, f_log_lh, f_proposal, f_log_gprime)
    prop_covs = sampler.tune_proposal(100, f_prior)
    N_samples = int(2*1e4)
    nu = N_samples # this means no adaptive update
    N_burn_in = 3000
    swap_interval = 1 # propose chain swaps every step
    sd_arr = 2.4**2 / np.array(dim_list)
    prop_cov_arr = 2*noise_std**2 / beta_arr

    sigma_scale = 0.1
    prop_cov_arr *= sigma_scale

    """ 
    Now run 
    """
    sampler.initialize_chains(N_samples, N_burn_in, nu, f_prior, update_after_burn, swap_interval, prop_covs)
    chain_list = sampler.chain_list
    sampler.sample()
    chain_list = sampler.chain_list
    cold_chain = chain_list[0]
    best_chain_ind = np.argmax(cold_chain.log_probs) + 1
    map_x = cold_chain.samples[:, np.argmax(cold_chain.log_probs)]
    dim = int(map_x[0])
    map_coeff = map_x[1:dim+1]
    print('MAP coeff: {}'.format(map_coeff))
    cold_samples = cold_chain.samples
    print(1/cold_chain.params.beta)
    cold_samples = cold_samples[:, N_burn_in:]
    vals = np.zeros((tgrid.size, cold_samples.shape[1]))
    for i in range(cold_samples.shape[1]):
        dim = int(cold_samples[0, i])
        vals[:, i] = np.polyval(cold_samples[1:dim+1, i], tgrid)
    mean_val = np.mean(vals, axis=1)
    std_val = np.std(vals, axis=1)
    #ax.plot(tgrid, np.polyval(map_coeff, tgrid), 'k--', alpha=1, label='map')
    ax.plot(tgrid, mean_val, 'k--', alpha=1, label='mean')
    ax.fill_between(tgrid, mean_val-2*std_val, mean_val+2*std_val, alpha=0.2, label='2 std')
    ax.legend()
    ax.grid(True)


    fig.savefig(pics_folder + 'trans_d_poly_regression_data.pdf')

    log_p_ar_fig, fig_ax_list, dim_fig = sampler.cold_chain_diagnostic_plot()

    dim_fig.savefig(pics_folder + 'trans_d_poly_dim_hist.pdf')

    log_p_ar_fig.savefig(pics_folder + 'trans_d_poly_log_p_ar.pdf')


    plt.show()


#tune_sd()
#polynomial_regression()
example_comparison_script()
